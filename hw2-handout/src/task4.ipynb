{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import string\n",
    "from collections import Counter\n",
    "from liblinear.liblinearutil import *\n",
    "import scipy.sparse as sp\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Number of stopwords: 341\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Define paths\n",
    "train_root = 'data/yelp_reviews_train.json'\n",
    "test_root = 'data/yelp_reviews_test.json'\n",
    "val_root = 'data/yelp_reviews_dev.json'\n",
    "stopword_root = 'data/stopword.list'\n",
    "\n",
    "# Token pattern\n",
    "TOKEN_PATTERN = re.compile(r'\\b[a-z]+\\b')\n",
    "\n",
    "with open(stopword_root, 'r') as f:\n",
    "    stopwords = {line.strip() for line in f}\n",
    "print(f\"Number of stopwords: {len(stopwords)}\")\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpReviewsDataset(Dataset):\n",
    "    def __init__(self, features, labels=None):\n",
    "        \"\"\"\n",
    "        Custom Dataset for Yelp reviews using precomputed TF-IDF features.\n",
    "\n",
    "        Args:\n",
    "            features (np.ndarray): Precomputed TF-IDF feature matrix.\n",
    "            labels (np.ndarray, optional): Corresponding labels (class indices).\n",
    "        \"\"\"\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        if labels is not None:\n",
    "            self.labels = torch.tensor(labels, dtype=torch.long)  # Use long for class indices\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.features[idx], self.labels[idx]\n",
    "        else:\n",
    "            return self.features[idx]  # Return only features if no labels are provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data from data/yelp_reviews_train.json: 1255353it [00:14, 86088.97it/s]\n",
      "Loading data from data/yelp_reviews_dev.json: 157010it [00:01, 96243.85it/s]\n",
      "Loading data from data/yelp_reviews_test.json: 156901it [00:02, 75193.83it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean_and_tokenize(text, stopwords, translator, token_pattern):\n",
    "    text = text.lower().translate(translator)\n",
    "    tokens = token_pattern.findall(text)\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "# Function to load training data and extract texts and stars\n",
    "def load_training_data(file_path):\n",
    "    texts, stars = [], []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in tqdm(f, desc=f\"Loading data from {file_path}\"):\n",
    "            review = json.loads(line)\n",
    "            texts.append(review['text'])\n",
    "            stars.append(review['stars'])\n",
    "    return texts, np.array(stars)\n",
    "\n",
    "# Function to load text data (for test and validation sets)\n",
    "def load_text_data(file_path):\n",
    "    texts = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in tqdm(f, desc=f\"Loading data from {file_path}\"):\n",
    "            review = json.loads(line)\n",
    "            texts.append(review['text'])\n",
    "    return texts\n",
    "\n",
    "# Function to tokenize text data\n",
    "def tokenize_texts(texts, stopwords, translator, token_pattern):\n",
    "    return [clean_and_tokenize(text, stopwords, translator, token_pattern) for text in tqdm(texts, desc=\"Tokenizing texts\")]\n",
    "\n",
    "\n",
    "# Define function to create TF-IDF feature matrix\n",
    "def create_tfidf_matrix(texts, max_features=2000):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=stopwords,  # Use the loaded stopwords\n",
    "        max_features=max_features,  # Maximum number of features to keep\n",
    "        lowercase=True,  # Convert texts to lowercase\n",
    "        token_pattern=r'\\b[a-z]+\\b'  # Token pattern to use for extracting words\n",
    "    )\n",
    "    # Fit the vectorizer on the texts and return the resulting TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts).toarray()\n",
    "    return tfidf_matrix, vectorizer\n",
    "\n",
    "# Load and preprocess the texts\n",
    "train_texts, stars = load_training_data(train_root)\n",
    "val_texts = load_text_data(val_root)\n",
    "test_texts = load_text_data(test_root)\n",
    "\n",
    "print(\"Making the TF-IDF matrix for the train set\")\n",
    "# Create the TF-IDF feature matrix for train, val, and test sets\n",
    "features_tfidf_train, tfidf_vectorizer = create_tfidf_matrix(train_texts, max_features=2000)\n",
    "features_tfidf_val = tfidf_vectorizer.transform(val_texts).toarray()\n",
    "features_tfidf_test = tfidf_vectorizer.transform(test_texts).toarray()\n",
    "\n",
    "# Print the shape of the matrices\n",
    "print(f\"TF-IDF Train Shape: {features_tfidf_train.shape}\")\n",
    "print(f\"TF-IDF Val Shape: {features_tfidf_val.shape}\")\n",
    "print(f\"TF-IDF Test Shape: {features_tfidf_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert stars to labels\n",
    "# Adjust star ratings from 1-5 to 0-4\n",
    "labels_train = stars - 1  # Assuming stars are [1, 2, 3, 4, 5]\n",
    "#labels_val = one_hot_encode_labels(stars[len(features_tfidf_train):])  # Validation labels\n",
    "\n",
    "# Create datasets for training, validation, and test\n",
    "train_dataset = YelpReviewsDataset(features=features_tfidf_train, labels=labels_train)  # Training set with labels\n",
    "val_dataset = YelpReviewsDataset(features=features_tfidf_val)  # Validation set without labels\n",
    "test_dataset = YelpReviewsDataset(features=features_tfidf_test)  # Test set without labels\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train features shape: {features_tfidf_train.shape}\")\n",
    "print(f\"Train labels shape: {labels_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train features shape: {features_tfidf_train.shape}\")\n",
    "print(f\"Train labels shape: {labels_train.shape}\")\n",
    "print(f\"Validation features shape: {features_tfidf_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model with a hidden layer\n",
    "class RMLR(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(RMLR, self).__init__()\n",
    "        self.hidden = nn.Linear(num_features, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model = RMLR(num_features=features_tfidf_train.shape[1], num_classes=5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "\n",
    "# Update batch size and epochs\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "# Training loop with RMSE calculation\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    squared_error = 0.0  # For RMSE calculation\n",
    "    \n",
    "    for features, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * features.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # Compute soft predictions (expected ratings)\n",
    "        probabilities = nn.functional.softmax(outputs, dim=1)\n",
    "        class_indices = torch.arange(1, 6).float().to(device)  # Ratings from 1 to 5\n",
    "        expected_ratings = torch.matmul(probabilities, class_indices)\n",
    "        true_ratings = labels.float() + 1  # Adjust labels back to 1-5\n",
    "        \n",
    "        # Accumulate squared errors\n",
    "        squared_error += torch.sum((expected_ratings - true_ratings) ** 2).item()\n",
    "    \n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_accuracy = 100 * correct / total_samples\n",
    "    rmse = np.sqrt(squared_error / total_samples)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%, RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for validation (dev) and test sets\n",
    "def generate_predictions(model, data_loader, output_file):\n",
    "    model.eval()\n",
    "    hard_predictions = []\n",
    "    soft_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features in tqdm(data_loader, desc=\"Generating predictions\"):\n",
    "            features = features.to(device)\n",
    "            outputs = model(features)\n",
    "            probabilities = nn.functional.softmax(outputs, dim=1)\n",
    "\n",
    "            # Hard predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            hard_predictions.extend((predicted + 1).cpu().numpy())  # Adjusting back to 1-5\n",
    "\n",
    "            # Soft predictions\n",
    "            class_indices = torch.arange(1, 6).float().to(device)\n",
    "            soft_pred = torch.matmul(probabilities, class_indices)\n",
    "            soft_predictions.extend(soft_pred.cpu().numpy())\n",
    "\n",
    "    # Write predictions to file\n",
    "    with open(output_file, 'w') as f:\n",
    "        for hard_pred, soft_pred in zip(hard_predictions, soft_predictions):\n",
    "            f.write(f\"{hard_pred} {soft_pred:.4f}\\n\")\n",
    "\n",
    "# Generate predictions for validation (dev) set\n",
    "generate_predictions(model, val_loader, \"dev-predictions.txt\")\n",
    "\n",
    "# Generate predictions for test set\n",
    "generate_predictions(model, test_loader, \"test-predictions.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
